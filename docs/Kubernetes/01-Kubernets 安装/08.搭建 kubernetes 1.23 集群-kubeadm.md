> 本文所实践的 k8s 版本为当前最新版 v1.23.5，基于 CentOS 7.9 2009 系统，不保证其他操作系统及版本可以正确执行
> 图片来自网络以及官方文档 https://kubernetes.io/zh/docs/home/
> 部分概念仅为本人理解所写，如有错误欢迎指出


# 服务器
|主机名| IP | 角色|软件|
|--|--|--|--|
| k8s-master | 192.168.0.11 | master 主机 | kubeadm、kubelet、kubectl|
| k8s-node1  | 192.168.0.12 | node 节点 |kubeadm、kubelet|
| k8s-node2  | 192.168.0.13 | node 节点 |kubeadm、kubelet|
| k8s-node3  | 192.168.0.14 |  node 节点 |kubeadm、kubelet|

# 安装 kubernetes 集群
## 服务器环境初始化
所有节点初始化操作

### 主机名绑定
给每台主机设置主机名
```bash
hostnamectl set-hostname $hostname
```

编辑 `/etc/hosts` 文件，追加主机名解析规则，主机名不要用**下划线**字符
```bash
192.168.0.11      k8s-master
192.168.0.12      k8s-node1
192.168.0.13      k8s-node2
192.168.0.14      k8s-node3
```

### 关闭 selinux

```bash
setenforce 0 && sed -i 's/enforcing/disabled/g' /etc/selinux/config
```

### 关闭 firewalld

```bash
systemctl stop firewalld.service && systemctl disable firewalld.service
```
### 清空 iptables 规则

安装 iptables-services

```bash
yum install -y iptables-services
```
重启 iptables 服务
```bash
systemctl restart iptables.service && systemctl enable iptables.service
```
清空路由表规则
```bash
iptables -F && iptables -F -t nat && iptables -F -t mangle && iptables -F -t raw
```
保存规则
```bash
service iptables save
```

### 关闭 swap

```bash
swapoff -a
```
编辑 `/etc/fstab` 文件，如果有 swap 自动挂载，则将该行注释掉
```bash
# UUID=9ba6a188-d8e1-4983-9abe-ba4a29b1d138 swap swap defaults 0 0
```

### 开启内核模块

```shell
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh
modprobe nf_conntrack
modprobe nf_conntrack_ipv4
modprobe br_netfilter
modprobe overlay
EOF
```

```shell
sh /etc/sysconfig/modules/ipvs.modules
```

配置模块自动加载

```shell
cat > /etc/modules-load.d/k8s-modules.conf <<EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
nf_conntrack_ipv4
br_netfilter
overlay
EOF
```

```shell
systemctl enable systemd-modules-load && systemctl restart systemd-modules-load
```

### 内核调优

```bash
cat > /etc/sysctl.d/kubernetes.conf <<EOF
# 开启数据包转发功能（实现vxlan）
net.ipv4.ip_forward = 1
# 关闭 tcp_tw_recycle，否则和 NAT 冲突，会导致服务不通
net.ipv4.tcp_tw_recycle = 0
# 不允许将TIME-WAIT sockets重新用于新的TCP连接
net.ipv4.tcp_tw_reuse = 0
# 持久连接
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
# 关闭 swap 分区
vm.swappiness = 0
# iptables 对 bridge 的数据进行处理
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-arptables = 1
# socket 监听(listen)的 backlog 上限
net.core.somaxconn = 32768
# 最大跟踪连接数，默认 nf_conntrack_buckets * 4
net.netfilter.nf_conntrack_max = 1000000
# 计算当前的内存映射文件数。
vm.max_map_count = 655360
# 内核可分配的最大文件数
fs.file-max = 6553600
EOF
```
刷新生效
```bash
sysctl -p /etc/sysctl.d/kubernetes.conf
```

### 时间同步

```bash
yum install -y ntp
```

```bash
cat > /etc/ntp.conf << EOF
driftfile /var/lib/ntp/drift
restrict default nomodify notrap nopeer noquery
restrict 127.0.0.1 
restrict ::1
server 0.cn.pool.ntp.org
server 1.cn.pool.ntp.org
server 2.cn.pool.ntp.org
server 3.cn.pool.ntp.org
includefile /etc/ntp/crypto/pw
keys /etc/ntp/keys
disable monitor
EOF
```
```bash
systemctl stop chronyd && systemctl disable chronyd
```
```bash
systemctl start ntpd && systemctl enable ntpd
```
```bash
ntpdate -u 0.cn.pool.ntp.org
```
```bash
hwclock --systohc
```
```bash
echo '0 * * * * /usr/sbin/ntpdate -u 0.cn.pool.ntp.org >> /tmp/autontpdate 2>&1'  >> /var/spool/cron/root
```

### 安装 docker
```bash
yum install -y yum-utils device-mapper-persistent-data lvm2 \
&& yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo \
&& yum makecache fast \
&& yum install -y docker-ce-20.10.9-3.el7
```
启动 docker
```bash
systemctl start docker && systemctl enable docker
```
配置 docker 启动参数 `/etc/docker/daemon.json`，所有节点配置加速器和将 cgroupdriver 改为 systemd，并重启 docker 服务

```yaml
{
	"registry-mirrors": ["https://42h8kzrh.mirror.aliyuncs.com"],
	"exec-opts": ["native.cgroupdriver=systemd"]
}
```

```bash
systemctl restart docker
```

### 安装 kubelet、kubeadm
准备 `/etc/yum.repos.d/kubernetes.repo`   YUM 源

```bash
[k8s]
name=k8s
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
```

```shell
yum clean all && yum makecache fast
```

```bash
yum install -y kubelet-1.23.5 kubeadm-1.23.5
```

```bash
systemctl enable kubelet
```
### kubectl、kubeadm 命令补全

```shell
yum install -y bash-completion
echo 'source <(kubectl completion bash)' >> $HOME/.bashrc
echo 'source <(kubeadm completion bash)' >> $HOME/.bashrc
source $HOME/.bashrc
```

## master 节点操作

### kubeadm init 初始化

参考文档：https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/

```bash
kubeadm init \
--apiserver-advertise-address=172.29.9.10 \
--apiserver-bind-port=6443 \
--node-name=k8s-master \
--image-repository=registry.aliyuncs.com/google_containers \
--kubernetes-version=1.23.5 \
--service-cidr=10.2.0.0/16 \
--pod-network-cidr=10.244.0.0/16 \
--token=d550q5.22wp4ezqlsun3qdj \
--token-ttl=0
```

```yaml

--apiserver-advertise-address: API 服务器所公布的其正在监听的 IP 地址。
--apiserver-bind-port: API 服务器绑定的端口。
--node-name: 指定节点的名称。
--image-repository: 选择用于拉取镜像的容器仓库，默认值："k8s.gcr.io"
--kubernetes-version: 安装特定的 Kubernetes 版本。
--service-cidr: 为服务的虚拟 IP 地址另外指定 IP 地址段
--pod-network-cidr: 指明 pod 网络可以使用的 IP 地址段
--token: 这个令牌用于建立控制平面节点与工作节点间的双向通信
--token-ttl: 令牌被自动删除之前的持续时间（例如 1 s，2 m，3 h）。如果设置为 '0'，则令牌将永不过期
```

运行成功后，返回以下内容

```shell
[init] Using Kubernetes version: v1.23.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.2.0.1 192.168.0.11]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.0.11 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.0.11 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 7.004743 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: d550q5.22wp4ezqlsun3qdj
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.11:6443 --token d550q5.22wp4ezqlsun3qdj \
	--discovery-token-ca-cert-hash sha256:8b8fca2af29e39ae63f26ca6bdea3fac0ee9808b601f2737ef9984fa41d4fc7c 
```

通过输出日志，可以看到 kubeadm 初始化中做了很多事

1. 生成一个自签名的 CA 证书来为集群中的每一个组件建立身份标识。 用户可以通过将其放入 `--cert-dir` 配置的证书目录中（默认为 `/etc/kubernetes/pki`） 来提供他们自己的 CA 证书以及/或者密钥。 注意：证书有效期为一年。

2. 将 kubeconfig 文件写入 `/etc/kubernetes/` 目录以便 kubelet、控制器管理器和调度器用来连接到 API 服务器，它们每一个都有自己的身份标识，同时生成一个名为 `admin.conf` 的独立的 kubeconfig 文件，用于管理操作。

3. 为 API 服务器、控制器管理器和调度器生成静态 Pod 的清单文件。假使没有提供一个外部的 etcd 服务的话，也会为 etcd 生成一份额外的静态 Pod 清单文件。

   静态 Pod 的清单文件被写入到 `/etc/kubernetes/manifests` 目录; kubelet 会监视这个目录以便在系统启动的时候创建 Pod。

   一旦 Master 的 Pod 都运行起来， `kubeadm init` 的工作流程就继续往下执行。

4. 对 Master 节点打上污点标记以便不会在它上面运行其它的工作负载。
5. 生成令牌，将来其他节点可使用该令牌向 Master 节点注册自己。
6. 通过 API 服务器安装一个 DNS 服务器 (CoreDNS) 和 kube-proxy 附加组件。 

### 配置 kubectl 工具

```shell
mkdir -p $HOME/.kube \
&& cp -i /etc/kubernetes/admin.conf $HOME/.kube/config \
&& chown $(id -u):$(id -g) $HOME/.kube/config
```


### 查看集群组件状态

```shell
# kubectl get componentstatuses
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
controller-manager   Healthy   ok                              
scheduler            Healthy   ok                              
etcd-0               Healthy   {"health":"true","reason":""} 
```

### 部署 Flannel 网络插件

```shell
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

等待 flannel 插件启动成功，查看组件 pod 是否全部 ready

```shell
[root@k8s-master ~]# kubectl get pods -n kube-system 
NAME                                 READY   STATUS    RESTARTS   AGE
coredns-6d8c4cb4d-n74wq              1/1     Running   0          66m
coredns-6d8c4cb4d-pwmrg              1/1     Running   0          66m
etcd-k8s-master                      1/1     Running   0          67m
kube-apiserver-k8s-master            1/1     Running   0          67m
kube-controller-manager-k8s-master   1/1     Running   0          67m
kube-flannel-ds-8c2kc                1/1     Running   0          5m25s
kube-proxy-qp4zs                     1/1     Running   0          66m
kube-scheduler-k8s-master            1/1     Running   0          67m
```

查看网卡

```shell
# ifconfig cni0
cni0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
        inet 10.3.0.1  netmask 255.255.255.0  broadcast 10.3.0.255
        inet6 fe80::689d:beff:fecb:e5fd  prefixlen 64  scopeid 0x20<link>
        ether 6a:9d:be:cb:e5:fd  txqueuelen 1000  (Ethernet)
        RX packets 795  bytes 56315 (54.9 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 467  bytes 63109 (61.6 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

# ifconfig flannel.1
flannel.1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
        inet 10.3.0.0  netmask 255.255.255.255  broadcast 0.0.0.0
        inet6 fe80::4e7:a4ff:fe3f:220a  prefixlen 64  scopeid 0x20<link>
        ether 06:e7:a4:3f:22:0a  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 8 overruns 0  carrier 0  collisions 0
```

### 查看 master 节点状态

```shell
# kubectl get nodes
NAME         STATUS   ROLES                  AGE   VERSION
k8s-master   Ready    control-plane,master   71m   v1.23.5
```

如果初始化遇到问题，尝试使用下面的命令清理,再重新初始化

```shell
kubeadm reset
ifconfig cni0 down && ip link delete cni0
ifconfig flannel.1 down && ip link delete flannel.1
rm -rf /var/lib/cni
```

## node 节点操作

### kubeadm join 加入集群

需要加入集群的 node 节点执行以下命令

```shell
kubeadm join 172.29.9.10:6443 --token d550q5.22wp4ezqlsun3qdj \
	--discovery-token-ca-cert-hash sha256:6458d3fae04b0536470b9ef182fa666510dc439f69ac74d7f7af3659eec5daaa 
```

返回值

```shell
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

## 确认集群状态

等待 node 节点上的 pod 全部启动成功

```shell
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES                  AGE     VERSION
k8s-master   Ready    control-plane,master   81m     v1.23.5
k8s-node1    Ready    <none>                 6m25s   v1.23.5
k8s-node2    Ready    <none>                 6m26s   v1.23.5
k8s-node3    Ready    <none>                 6m26s   v1.23.5
```





